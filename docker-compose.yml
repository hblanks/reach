version: '3.4'

x-airflow-image: &airflow-image
  160358319781.dkr.ecr.eu-west-1.amazonaws.com/uk.ac.wellcome/policytool:latest

x-airflow_env: &airflow-env
  AIRFLOW_HOME: /airflow



  # S3 prefix for all datalabs operators.
  # (in `core` because we must be in an existing section of the cfg)
  AIRFLOW__CORE__REACH_S3_PREFIX: "s3://datalabs-dev/reach-airflow" # No trailing /!
  AIRFLOW__CORE__OPENRESEARCH_S3_PREFIX: "s3://datalabs-dev/airflow" # No trailing /!
  # AIRFLOW__CORE__DATALABS_ALTERYX_PREFIX: "s3://datalabs-staging/alteryx" # No trailing /!

  # dev creds encryption key, cf.
  # https://airflow.readthedocs.io/en/stable/howto/secure-connections.html
  # (but not used; everything's from env atm)
  AIRFLOW__CORE__FERNET_KEY: "${AIRFLOW_FERNET_KEY}"

  # Wait 20s to connect to PostgreSQL, in case it's coming up fresh.
  AIRFLOW__CORE__SQL_ALCHEMY_CONN: "postgresql://postgres:development@postgres:5432/airflow?connect_timeout=20"
  AIRFLOW__CELERY__BROKER_URL: "redis://redis:6379/0"
  AIRFLOW__CELERY__RESULT_BACKEND: "db+postgresql://postgres:development@postgres:5432/airflow_celery_results?connect_timeout=20"

  AWS_ACCESS_KEY_ID: "${AWS_ACCESS_KEY_ID}"
  AWS_SECRET_ACCESS_KEY: "${AWS_SECRET_ACCESS_KEY}"
  AWS_SESSION_TOKEN: "${AWS_SESSION_TOKEN}"

  DATALABS_AIRFLOW_VERSION: "localdev"
  SENTRY_DSN: "${SENTRY_DSN}"

  DATABASE_URL: "postgresql://postgres:development@postgres:5432/wsf_scraping"

x-airflow-volumes: &airflow-volumes
  - ./policytool/airflow/airflow.cfg:/airflow/airflow.cfg
  - ./policytool/airflow/initdb.sh:/airflow/initdb.sh
  - ./policytool:/src/policytool/policytool
  - ./build/airflow.db:/airflow/db

# Restart airflow several times in case the DB wasn't present.
# (Yes, we could netcat, but postgres can accept TCP conns before
# accepting queries...so health checking is not so easy here.)
x-airflow-restart: &airflow-restart
  condition: on-failure
  delay: 2s
  max_attempts: 4
  window: 3s


services:
  postgres:
    image: postgres:11.2-alpine
    ports:
      - 127.0.0.1:5432:5432
    environment:
      POSTGRES_PASSWORD: development
    volumes:
      - ./build/airflow.db:/var/lib/postgresql/data
      - ./policytool/airflow/initdb:/docker-entrypoint-initdb.d

  redis:
    image: redis:5.0.3-alpine
    ports:
      - 127.0.0.1:6379:6379
    command:
      - redis-server
      - --appendonly
      - "yes"
    volumes:
      - ./build/airflow.redis:/data

  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:7.2.0
    environment:
      - discovery.type=single-node
      - "ES_JAVA_OPTS=-Xms2g -Xmx2g"
    ports:
      - 127.0.0.1:9200:9200
    volumes:
      - esdata:/usr/share/elasticsearch/data
  # Uncomment if you want 100% CPU usage.
  #
  # kibana:
  #   image: docker.elastic.co/kibana/kibana:7.0.0
  #   ports:
  #     - 127.0.0.1:5601:5601
  #   environment:
  #     ELASTICSEARCH_HOSTS: "http://elasticsearch:9200"

  airflow-scheduler:
    image: *airflow-image
    environment:
      <<: *airflow-env
      POSTGRES_DSN: "postgresql://postgres:development@postgres:5432/airflow?connect_timeout=20"
    command:
      - /src/policytool/policytool/scraper/pg_isready.py
      - sh
      - "-cx"
      - "/airflow/initdb.sh && exec airflow scheduler"
    volumes: *airflow-volumes
    depends_on: ["postgres", "redis"]

  airflow-web:
    image: *airflow-image
    ports:
      - 127.0.0.1:8080:8080
    environment:
      <<: *airflow-env
      POSTGRES_DSN: "postgresql://postgres:development@postgres:5432/airflow?connect_timeout=20"
    command:
      # airflow webserver must be brought up only after the DB is
      # reliably serving connections, and after key parts of the DDL are
      # in place.
      # Thus pg_isready.py, and with a longer success-secs than
      # airflow-scheduler.
      - /src/policytool/policytool/scraper/pg_isready.py
      - "--timeout=30"
      - "--success-secs=8"  # allow extra time for initdb to get started
      - airflow
      - webserver
      - "-p"
      - "8080"
    volumes: *airflow-volumes
    depends_on: ["postgres", "redis"]

  airflow-worker:
    image: *airflow-image
    environment:
      <<: *airflow-env
      POSTGRES_DSN: "postgresql://postgres:development@postgres:5432/airflow?connect_timeout=20"
    command:
      - airflow
      - worker
    volumes: *airflow-volumes
    depends_on: ["postgres", "redis"]

  web:
    image: *airflow-image
    ports:
      - 127.0.0.1:8081:8081
    environment:
      AWS_ACCESS_KEY_ID: "${AWS_ACCESS_KEY_ID}"
      AWS_SECRET_ACCESS_KEY: "${AWS_SECRET_ACCESS_KEY}"
      SENTRY_DSN: "${SENTRY_DSN}"
      STATIC_ROOT: /build/web/static
      ELASTICSEARCH_HOST: "http://elasticsearch:9200"
      # DATABASE_URL: "postgres://postgres:postgres@articles-db:5432/wsf_scraping"

    command:
      - gunicorn
      - --bind=0.0.0.0:8081
      - --reload
      - policytool.web.wsgi:application
    volumes:
      - ./policytool/web:/src/policytool/policytool/web/
      - ./policytool/scraper:/src/policytool/policytool/scraper/
      - ./build/web:/build/web

  # TODO: run gulp watch with the right volumes in place
  # and update the web image to serve static from build/static
  # by either an env or complicated volumes
  gulp-watch:
    image: policytool-web-build:latest
    command:
      - gulp
      - watch
    volumes:
      - ./build/web/static:/src/build/static
      - ./policytool/web/gulpfile.js:/src/gulpfile.js
      - ./policytool/web/static:/src/static
    # Signal handling in gulp? Not so much...
    stop_signal: SIGKILL

volumes:
  esdata:
    driver: local
